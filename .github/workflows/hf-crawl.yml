name: HF Crawl (matrix + mirror + org fallback)

on:
  workflow_dispatch:
    inputs:
      max_per_tag:
        description: "Cap per pipeline tag (rows)"
        default: "30000"
      overall_cap:
        description: "Overall safety cap (rows)"
        default: "500000"
      tags:
        description: "Comma-separated pipeline tags"
        default: "text-generation,text2text-generation,question-answering,summarization,translation,conversational,token-classification,text-classification,zero-shot-classification,table-question-answering,sentence-similarity,feature-extraction,fill-mask,image-classification,object-detection,image-segmentation,depth-estimation,image-to-text,text-to-image,image-to-image,unconditional-image-generation,super-resolution,visual-question-answering,document-question-answering,image-feature-extraction,automatic-speech-recognition,text-to-speech,audio-to-audio,voice-activity-detection,audio-classification,speaker-diarization,keyword-spotting,tabular-classification,tabular-regression,reinforcement-learning,time-series-forecasting"

jobs:
  sweep:
    name: sweep-${{ matrix.os }}-${{ matrix.base_alias }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        base: ["https://huggingface.co/api/models", "https://hf-mirror.com/api/models"]
        include:
          - base: "https://huggingface.co/api/models"
            base_alias: main
          - base: "https://hf-mirror.com/api/models"
            base_alias: mirror
    runs-on: ${{ matrix.os }}
    timeout-minutes: 240
    steps:
      - uses: actions/checkout@v4
      - name: Install jq
        if: startsWith(matrix.os,'ubuntu') || startsWith(matrix.os,'macos')
        run: |
          if [ "$RUNNER_OS" = "Linux" ]; then sudo apt-get update -y && sudo apt-get install -y jq; fi
          if [ "$RUNNER_OS" = "macOS" ]; then brew install jq; fi
      - name: Install jq (Windows)
        if: startsWith(matrix.os,'windows')
        run: choco install jq -y
      - name: Crawl (search + cursors)
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          MAX_PER_TAG: ${{ github.event.inputs.max_per_tag }}
          OVERALL_CAP: ${{ github.event.inputs.overall_cap }}
          TAGS: ${{ github.event.inputs.tags }}
          BASE: ${{ matrix.base }}
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p out
          : > out/raw.jsonl

          UA="COS-Frontier-GHA/3.0"
          AUTH=()
          if [ -n "${HF_TOKEN:-}" ]; then AUTH=(-H "Authorization: Bearer ${HF_TOKEN}"); fi

          IFS=',' read -ra TAGS_ARR <<< "${TAGS}"
          overall=0

          for TAG in "${TAGS_ARR[@]}"; do
            echo "=== TAG: ${TAG} @ ${BASE} ==="
            cursor=""
            fetched=0
            while :; do
              URL="${BASE}?limit=100&full=true&sort=downloads&direction=-1&search=$(printf "pipeline_tag:%s" "$TAG" | jq -sRr @uri)"
              [ -n "$cursor" ] && URL="${URL}&cursor=$(printf "%s" "$cursor" | jq -sRr @uri)"
              headers=$(mktemp); body=$(mktemp)
              code=$(curl -sS -m 60 -D "$headers" -H "User-Agent: $UA" "${AUTH[@]}" "$URL" -o "$body" -w '%{http_code}') || code=000
              if [ "$code" != "200" ]; then echo "[warn] HTTP $code"; rm -f "$headers" "$body"; break; fi
              cnt=$(jq 'length' < "$body" || echo 0)
              if [ "$cnt" -eq 0 ]; then rm -f "$headers" "$body"; break; fi
              jq -c '.[]' < "$body" >> out/raw.jsonl
              fetched=$((fetched + cnt)); overall=$((overall + cnt))
              cursor=$(awk -F': ' 'BEGIN{IGNORECASE=1} tolower($1)=="x-next-page-cursor"{gsub(/\r/,"",$2); print $2}' "$headers")
              rm -f "$headers" "$body"
              [ "$fetched" -ge "$MAX_PER_TAG" ] && echo "[cap] tag cap hit" && break
              [ "$overall" -ge "$OVERALL_CAP" ] && echo "[cap] overall cap hit" && break
              [ -z "$cursor" ] && break
              sleep 0.1
            done
            [ "$overall" -ge "$OVERALL_CAP" ] && break
          done

          # Dedup this job's results
          jq -s '
            map({mid: (.modelId//.id//""), row: .})
            | map(select(.mid != "")) 
            | unique_by(.mid)
            | map(.row)
          ' out/raw.jsonl > out/raw_dedup.json
          jq -c '.[]' out/raw_dedup.json > out/raw_dedup.jsonl
          rm -f out/raw_dedup.json

          echo "[info] job rows: $(wc -l < out/raw_dedup.jsonl)"

      - name: Upload per-job artifact
        uses: actions/upload-artifact@v4
        with:
          name: chunk-${{ matrix.os }}-${{ matrix.base_alias }}
          path: out/raw_dedup.jsonl
          if-no-files-found: ignore

  merge:
    needs: [sweep]
    runs-on: ubuntu-latest
    timeout-minutes: 120
    steps:
      - uses: actions/checkout@v4
      - name: Install jq
        run: |
          sudo apt-get update -y
          sudo apt-get install -y jq moreutils
      - name: Download all chunks
        uses: actions/download-artifact@v4
        with:
          path: chunks
      - name: Merge & dedupe chunks
        run: |
          set -euo pipefail
          mkdir -p out
          : > out/hf_models_raw.jsonl
          find chunks -type f -name '*.jsonl' -print0 | xargs -0 -I{} bash -c 'cat "{}" >> out/hf_models_raw.jsonl'
          echo "[info] merged lines: $(wc -l < out/hf_models_raw.jsonl)"

          # ORG SWEEP (no-search fallback) â€” large list
          BASE="https://huggingface.co/api/models"
          UA="COS-Frontier-GHA/3.0"
          AUTH=()
          if [ -n "${{ secrets.HF_TOKEN }}" ]; then AUTH=(-H "Authorization: Bearer ${{ secrets.HF_TOKEN }}"); fi

          ORGS="mistralai meta-llama Qwen deepseek-ai EleutherAI tiiuae google microsoft allenai nvidia stabilityai bigscience databricks togethercomputer baichuan-inc 01-ai xverse openaccess-ai-collective mlx-community Salesforce tencent OpenAssistant mlc-ai HuggingFaceH4 HuggingFaceM4 laion uclanlp stanfordnlp princeton-nlp OpenGVLab IDEA-CCNL openbmb baidu TencentARC kakaobrain ai-forever bigcode codellama codefuse-ai TheBloke bartowski NousResearch mlabonne philschmid RWKV declare-lab cerebras mosaicml nomic-ai openlm-research openbuddy openrouter aws-neuron OLMo-LM stabilityai-stablelm cerebras-GPT databricks-dolly vllm-project openbmb Minfo-NLP yandex ai21 SalesforceAIResearch snowflake "

          : > out/author_append.jsonl
          for org in $ORGS; do
            url="${BASE}?limit=100&full=true&author=$(printf "%s" "$org" | jq -sRr @uri)"
            body=$(mktemp)
            code=$(curl -sS -m 60 -H "User-Agent: $UA" "${AUTH[@]}" "$url" -o "$body" -w '%{http_code}') || code=000
            if [ "$code" = "200" ]; then jq -c '.[]' < "$body" >> out/author_append.jsonl || true; fi
            rm -f "$body"; sleep 0.07
          done

          # merge + dedupe final
          (cat out/hf_models_raw.jsonl out/author_append.jsonl) > out/_merge_all.jsonl

          jq -s '
            map({mid: (.modelId//.id//""), row: .})
            | map(select(.mid != "")) 
            | unique_by(.mid)
            | map(.row)
          ' <(jq -c '.' out/_merge_all.jsonl) > out/hf_models_raw_dedup.json
          jq -c '.[]' out/hf_models_raw_dedup.json > out/hf_models_raw_dedup.jsonl
          rm -f out/hf_models_raw_dedup.json out/_merge_all.jsonl out/author_append.jsonl

          echo "[info] FINAL rows: $(wc -l < out/hf_models_raw_dedup.jsonl)"

      - name: Make CSV (no Python)
        run: |
          set -euo pipefail
          echo "modelId,author,pipeline_tag,createdAt,lastModified,downloads,likes" > out/hf_models_dedup.csv
          jq -r '
            [
              (.modelId // .id // ""),
              (.author // ""),
              (.pipeline_tag // ""),
              (.createdAt // ""),
              (.lastModified // ""),
              (.downloads // 0),
              (.likes // 0)
            ] | @csv
          ' out/hf_models_raw_dedup.jsonl >> out/hf_models_dedup.csv
          echo "[info] CSV rows: $(($(wc -l < out/hf_models_dedup.csv)-1))"

      - name: Upload final artifacts
        uses: actions/upload-artifact@v4
        with:
          name: hf-crawl-results
          path: |
            out/hf_models_raw.jsonl
            out/hf_models_raw_dedup.jsonl
            out/hf_models_dedup.csv
          if-no-files-found: error
