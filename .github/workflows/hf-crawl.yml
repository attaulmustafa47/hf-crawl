name: HF Crawl (curl+jq)

on:
  workflow_dispatch:  # ← this enables the “Run workflow” button

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 360
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install jq
        run: |
          sudo apt-get update -y
          sudo apt-get install -y jq

      - name: Crawl Hugging Face via REST
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}   # optional
          MAX_PER_TAG: "15000"
          OVERALL_CAP: "120000"
          TAGS: >-
            text-generation,text2text-generation,question-answering,summarization,translation,conversational,
            token-classification,text-classification,zero-shot-classification,table-question-answering,
            sentence-similarity,feature-extraction,fill-mask,image-classification,object-detection,
            image-segmentation,depth-estimation,image-to-text,text-to-image,image-to-image,
            unconditional-image-generation,super-resolution,visual-question-answering,
            document-question-answering,image-feature-extraction,automatic-speech-recognition,
            text-to-speech,audio-to-audio,voice-activity-detection,audio-classification,
            speaker-diarization,keyword-spotting,tabular-classification,tabular-regression,
            reinforcement-learning,time-series-forecasting
        run: |
          set -euo pipefail
          mkdir -p out
          : > out/hf_models_raw.jsonl

          BASE="https://huggingface.co/api/models"
          UA="COS-Frontier-GHA/1.0"
          AUTH=()
          if [ -n "${HF_TOKEN:-}" ]; then AUTH=(-H "Authorization: Bearer ${HF_TOKEN}"); fi

          IFS=',' read -ra TAGS_ARR <<< "${TAGS}"
          overall=0

          for TAG in "${TAGS_ARR[@]}"; do
            echo "=== TAG: ${TAG} ==="
            cursor=""
            fetched=0
            while :; do
              URL="${BASE}?limit=100&full=true&sort=downloads&direction=-1&search=$(printf "pipeline_tag:%s" "$TAG" | jq -sRr @uri)"
              [ -n "$cursor" ] && URL="${URL}&cursor=$(printf "%s" "$cursor" | jq -sRr @uri)"

              headers=$(mktemp)
              body=$(mktemp)
              code=$(curl -sS -m 60 -D "$headers" -H "User-Agent: $UA" "${AUTH[@]}" "$URL" -o "$body" -w '%{http_code}')
              if [ "$code" != "200" ]; then
                echo "[warn] HTTP $code for $TAG"; rm -f "$headers" "$body"; break
              fi

              count=$(jq 'length' < "$body")
              if [ "$count" -eq 0 ]; then rm -f "$headers" "$body"; break; fi

              jq -c '.[]' < "$body" >> out/hf_models_raw.jsonl
              fetched=$((fetched + count))
              overall=$((overall + count))

              cursor=$(awk -F': ' 'BEGIN{IGNORECASE=1} tolower($1)=="x-next-page-cursor"{gsub(/\r/,"",$2); print $2}' "$headers")
              rm -f "$headers" "$body"

              [ "$fetched" -ge "$MAX_PER_TAG" ] && echo "[cap] $TAG reached $MAX_PER_TAG" && break
              [ "$overall" -ge "$OVERALL_CAP" ] && echo "[cap] overall reached $OVERALL_CAP" && break
              [ -z "$cursor" ] && break
              sleep 0.1
            done
            [ "$overall" -ge "$OVERALL_CAP" ] && break
          done

          echo "[info] RAW lines: $(wc -l < out/hf_models_raw.jsonl)"

          # dedupe by modelId/id
          jq -s '
            map({mid: (.modelId//.id//""), row: .})
            | map(select(.mid != "")) 
            | unique_by(.mid)
            | map(.row)
          ' out/hf_models_raw.jsonl > out/hf_models_raw_dedup.json
          jq -c '.[]' out/hf_models_raw_dedup.json > out/hf_models_raw_dedup.jsonl
          rm -f out/hf_models_raw_dedup.json
          echo "[info] DEDUP lines: $(wc -l < out/hf_models_raw_dedup.jsonl)"

      - name: Make CSV (no Python)
        run: |
          set -euo pipefail
          echo "modelId,author,pipeline_tag,createdAt,lastModified,downloads,likes" > out/hf_models_dedup.csv
          jq -r '
            [
              (.modelId // .id // ""),
              (.author // ""),
              (.pipeline_tag // ""),
              (.createdAt // ""),
              (.lastModified // ""),
              (.downloads // 0),
              (.likes // 0)
            ] | @csv
          ' out/hf_models_raw_dedup.jsonl >> out/hf_models_dedup.csv
          echo "[info] CSV rows: $(($(wc -l < out/hf_models_dedup.csv)-1))"

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: hf-crawl-results
          path: |
            out/hf_models_raw.jsonl
            out/hf_models_raw_dedup.jsonl
            out/hf_models_dedup.csv
          if-no-files-found: error

