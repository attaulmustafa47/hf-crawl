name: HF Crawl (curl+jq)

on:
  workflow_dispatch:

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 360
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install jq
        run: |
          sudo apt-get update -y
          sudo apt-get install -y jq

      - name: Crawl Hugging Face via REST (search + cursors)
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}        # optional
          MAX_PER_TAG: "15000"
          OVERALL_CAP: "200000"
          TAGS: "text-generation,text2text-generation,question-answering,summarization,translation,conversational,token-classification,text-classification,zero-shot-classification,table-question-answering,sentence-similarity,feature-extraction,fill-mask,image-classification,object-detection,image-segmentation,depth-estimation,image-to-text,text-to-image,image-to-image,unconditional-image-generation,super-resolution,visual-question-answering,document-question-answering,image-feature-extraction,automatic-speech-recognition,text-to-speech,audio-to-audio,voice-activity-detection,audio-classification,speaker-diarization,keyword-spotting,tabular-classification,tabular-regression,reinforcement-learning,time-series-forecasting"
        run: |
          set -euo pipefail
          mkdir -p out
          : > out/hf_models_raw.jsonl

          BASE="https://huggingface.co/api/models"
          UA="COS-Frontier-GHA/1.1"
          AUTH=()
          if [ -n "${HF_TOKEN:-}" ]; then AUTH=(-H "Authorization: Bearer ${HF_TOKEN}"); fi

          IFS=',' read -ra TAGS_ARR <<< "${TAGS}"
          overall=0

          for TAG in "${TAGS_ARR[@]}"; do
            echo "=== TAG: ${TAG} ==="
            cursor=""
            fetched=0
            while :; do
              URL="${BASE}?limit=100&full=true&sort=downloads&direction=-1&search=$(printf "pipeline_tag:%s" "$TAG" | jq -sRr @uri)"
              [ -n "$cursor" ] && URL="${URL}&cursor=$(printf "%s" "$cursor" | jq -sRr @uri)"

              headers=$(mktemp); body=$(mktemp)
              code=$(curl -sS -m 60 -D "$headers" -H "User-Agent: $UA" "${AUTH[@]}" "$URL" -o "$body" -w '%{http_code}')
              if [ "$code" != "200" ]; then
                echo "[warn] HTTP $code for $TAG"; rm -f "$headers" "$body"; break
              fi

              count=$(jq 'length' < "$body")
              if [ "$count" -eq 0 ]; then rm -f "$headers" "$body"; break; fi

              jq -c '.[]' < "$body" >> out/hf_models_raw.jsonl
              fetched=$((fetched + count))
              overall=$((overall + count))

              cursor=$(awk -F': ' 'BEGIN{IGNORECASE=1} tolower($1)=="x-next-page-cursor"{gsub(/\r/,"",$2); print $2}' "$headers")
              rm -f "$headers" "$body"

              [ "$fetched" -ge "$MAX_PER_TAG" ] && echo "[cap] $TAG reached $MAX_PER_TAG" && break
              [ "$overall" -ge "$OVERALL_CAP" ] && echo "[cap] overall reached $OVERALL_CAP" && break
              [ -z "$cursor" ] && break
              sleep 0.1
            done
            [ "$overall" -ge "$OVERALL_CAP" ] && break
          done

          echo "[info] RAW lines after search: $(wc -l < out/hf_models_raw.jsonl)"

          # First dedupe by modelId/id
          jq -s '
            map({mid: (.modelId//.id//""), row: .})
            | map(select(.mid != "")) 
            | unique_by(.mid)
            | map(.row)
          ' out/hf_models_raw.jsonl > out/hf_models_raw_dedup.json
          jq -c '.[]' out/hf_models_raw_dedup.json > out/hf_models_raw_dedup.jsonl
          rm -f out/hf_models_raw_dedup.json

          echo "[info] DEDUP lines after search: $(wc -l < out/hf_models_raw_dedup.jsonl)"

      - name: Org sweep fallback (no-search; appends and re-dedupes)
        run: |
          set -euo pipefail
          BASE="https://huggingface.co/api/models"
          UA="COS-Frontier-GHA/1.1"
          AUTH=()
          if [ -n "${{ secrets.HF_TOKEN }}" ]; then AUTH=(-H "Authorization: Bearer ${{ secrets.HF_TOKEN }}"); fi

          ORGS="mistralai meta-llama Qwen deepseek-ai EleutherAI tiiuae google microsoft allenai nvidia stabilityai bigscience databricks togethercomputer baichuan-inc 01-ai xverse openaccess-ai-collective mlx-community Salesforce tencent OpenAssistant mlc-ai HuggingFaceH4 HuggingFaceM4 laion uclanlp stanfordnlp princeton-nlp OpenGVLab IDEA-CCNL openbmb baidu TencentARC kakaobrain ai-forever bigcode codellama codefuse-ai TheBloke bartowski NousResearch mlabonne philschmid RWKV declare-lab cerebras"

          # If previous step produced nothing, ensure file exists
          touch out/hf_models_raw.jsonl

          for org in $ORGS; do
            echo "[author] $org"
            url="${BASE}?limit=100&full=true&author=$(printf "%s" "$org" | jq -sRr @uri)"
            body=$(mktemp)
            code=$(curl -sS -m 60 -H "User-Agent: $UA" "${AUTH[@]}" "$url" -o "$body" -w '%{http_code}')
            if [ "$code" = "200" ]; then
              jq -c '.[]' < "$body" >> out/hf_models_raw.jsonl || true
            else
              echo "[warn] HTTP $code for author=$org"
            fi
            rm -f "$body"
            sleep 0.08
          done

          # De-dupe again after org sweep
          jq -s '
            map({mid: (.modelId//.id//""), row: .})
            | map(select(.mid != "")) 
            | unique_by(.mid)
            | map(.row)
          ' out/hf_models_raw.jsonl > out/hf_models_raw_dedup.json
          jq -c '.[]' out/hf_models_raw_dedup.json > out/hf_models_raw_dedup.jsonl
          rm -f out/hf_models_raw_dedup.json

          echo "[info] FINAL DEDUP lines: $(wc -l < out/hf_models_raw_dedup.jsonl)"

      - name: Make CSV (no Python)
        run: |
          set -euo pipefail
          echo "modelId,author,pipeline_tag,createdAt,lastModified,downloads,likes" > out/hf_models_dedup.csv
          jq -r '
            [
              (.modelId // .id // ""),
              (.author // ""),
              (.pipeline_tag // ""),
              (.createdAt // ""),
              (.lastModified // ""),
              (.downloads // 0),
              (.likes // 0)
            ] | @csv
          ' out/hf_models_raw_dedup.jsonl >> out/hf_models_dedup.csv
          echo "[info] CSV rows: $(($(wc -l < out/hf_models_dedup.csv)-1))"

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: hf-crawl-results
          path: |
            out/hf_models_raw.jsonl
            out/hf_models_raw_dedup.jsonl
            out/hf_models_dedup.csv
          if-no-files-found: error
