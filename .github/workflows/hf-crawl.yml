name: HF Crawl (matrix+alpha2+terms+orgs)

on:
  workflow_dispatch:
    inputs:
      max_per_tag:
        description: Cap per pipeline tag (rows) per job
        default: "70000"
      max_per_alpha:
        description: Cap per ONE-char search term (rows) per job
        default: "40000"
      max_per_alpha2:
        description: Cap per TWO-char search term (rows) per job
        default: "25000"
      max_per_term:
        description: Cap per common-term search (rows) per job
        default: "40000"
      overall_cap_per_job:
        description: Safety cap per job (rows)
        default: "300000"
      tags:
        description: Comma-separated pipeline tags for tag sweep
        default: "text-generation,text2text-generation,question-answering,summarization,translation,conversational,token-classification,text-classification,zero-shot-classification,table-question-answering,sentence-similarity,feature-extraction,fill-mask,image-classification,object-detection,image-segmentation,depth-estimation,image-to-text,text-to-image,image-to-image,unconditional-image-generation,super-resolution,visual-question-answering,document-question-answering,image-feature-extraction,automatic-speech-recognition,text-to-speech,audio-to-audio,voice-activity-detection,audio-classification,speaker-diarization,keyword-spotting,tabular-classification,tabular-regression,reinforcement-learning,time-series-forecasting"

jobs:
  sweep:
    name: sweep-${{ matrix.os }}-${{ matrix.base_alias }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        base: ["https://huggingface.co/api/models", "https://hf-mirror.com/api/models"]
        include:
          - base: "https://huggingface.co/api/models"
            base_alias: main
          - base: "https://hf-mirror.com/api/models"
            base_alias: mirror
    runs-on: ${{ matrix.os }}
    timeout-minutes: 330
    steps:
      - uses: actions/checkout@v4

      - name: Install jq (Linux/Mac)
        if: startsWith(matrix.os,'ubuntu') || startsWith(matrix.os,'macos')
        run: |
          if [ "$RUNNER_OS" = "Linux" ]; then sudo apt-get update -y && sudo apt-get install -y jq; fi
          if [ "$RUNNER_OS" = "macOS" ]; then brew install jq; fi

      - name: Install jq (Windows)
        if: startsWith(matrix.os,'windows')
        run: choco install jq -y

      - name: Crawl (tags + 1-char + 2-char + common-terms) on ${{ matrix.base_alias }}
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          BASE: ${{ matrix.base }}
          TAGS: ${{ github.event.inputs.tags }}
          MAX_PER_TAG: ${{ github.event.inputs.max_per_tag }}
          MAX_PER_ALPHA: ${{ github.event.inputs.max_per_alpha }}
          MAX_PER_ALPHA2: ${{ github.event.inputs.max_per_alpha2 }}
          MAX_PER_TERM: ${{ github.event.inputs.max_per_term }}
          OVERALL_CAP_PER_JOB: ${{ github.event.inputs.overall_cap_per_job }}
          COMMON_TERMS: "llama mistral qwen mixtral gemma yi phi gpt neo jurassic rwkv falcon opt llama2 llama-2 llama3 llama-3 qwen2 qwen2.5 baichuan deepseek xverse internlm glm chatglm vicuna wizard zephyr orca dolphin redpajama bloom dolly olmo olmoe alpaca open-llama tinyllama smol phi-3 phi-4 mistralai codellama codeqwen qwen2-math qwen2.5-math qwen2.5-coder qwen2-coder qwen-coder mistral-instruct mistral-nemo nemotron nemo nemo-guardrails yi-1.5 yi-chat yi-coder yi-vl llava mini-gpt4 minicpm cogagent polylm xlm roberta bert t5 flan-t5 m2m nllb mamba moa moa-ssm moondream medusa medusa2 xwin xwincoder xwinlm arctic plamo sft dpo orpo steerlm llama-guard llama-guard2 llama-guard-3 llama-guard-rail"
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p out
          : > out/raw.jsonl

          UA="COS-Frontier-GHA/9.0"
          AUTH=()
          if [ -n "${HF_TOKEN:-}" ]; then AUTH=(-H "Authorization: Bearer ${HF_TOKEN}"); fi

          fetch_loop() {
            # $1=searchTerm  $2=maxCap
            local term="$1"; local cap="$2"
            local cursor=""; local fetched=0
            while :; do
              local url="${BASE}?limit=100&full=true&sort=downloads&direction=-1&search=$(printf "%s" "$term" | jq -sRr @uri)"
              [ -n "$cursor" ] && url="${url}&cursor=$(printf "%s" "$cursor" | jq -sRr @uri)"
              headers=$(mktemp); body=$(mktemp)
              code=$(curl -sS -m 60 -D "$headers" -H "User-Agent: $UA" "${AUTH[@]}" "$url" -o "$body" -w '%{http_code}') || code=000
              if [ "$code" != "200" ]; then echo "[warn] HTTP $code term='${term}'"; rm -f "$headers" "$body"; break
              fi
              cnt=$(jq 'length' < "$body" || echo 0)
              if [ "$cnt" -eq 0 ]; then rm -f "$headers" "$body"; break; fi
              jq -c '.[]' < "$body" >> out/raw.jsonl
              fetched=$((fetched + cnt)); overall=$((overall + cnt))
              cursor=$(awk -F': ' 'BEGIN{IGNORECASE=1} tolower($1)=="x-next-page-cursor"{gsub(/\r/,"",$2); print $2}' "$headers")
              rm -f "$headers" "$body"
              [ "$fetched" -ge "$cap" ] && echo "[cap] hit for '${term}'" && break
              [ "$overall" -ge "$OVERALL_CAP_PER_JOB" ] && echo "[cap] job cap hit" && break
              [ -z "$cursor" ] && break
              sleep 0.05
            done
          }

          overall=0

          # --- 1) TAG SWEEP ---
          IFS=',' read -ra TAGS_ARR <<< "${TAGS}"
          for TAG in "${TAGS_ARR[@]}"; do
            echo "=== TAG: ${TAG} @ ${BASE} ==="
            fetch_loop "pipeline_tag:${TAG}" "${MAX_PER_TAG}"
            [ "$overall" -ge "$OVERALL_CAP_PER_JOB" ] && break
          done

          # --- 2) 1-CHAR SWEEP ---
          ALPHA1=(a b c d e f g h i j k l m n o p q r s t u v w x y z 0 1 2 3 4 5 6 7 8 9)
          for L in "${ALPHA1[@]}"; do
            echo "=== ALPHA1: ${L} @ ${BASE} ==="
            fetch_loop "${L}" "${MAX_PER_ALPHA}"
            [ "$overall" -ge "$OVERALL_CAP_PER_JOB" ] && break
          done

          # --- 3) 2-CHAR SWEEP (aa..zz, 00..99, a0..z9, 0a..9z) ---
          LETTERS=(a b c d e f g h i j k l m n o p q r s t u v w x y z)
          DIGITS=(0 1 2 3 4 5 6 7 8 9)

          # aa..zz
          for A in "${LETTERS[@]}"; do
            for B in "${LETTERS[@]}"; do
              term="${A}${B}"
              echo "=== ALPHA2 LL: ${term} @ ${BASE} ==="
              fetch_loop "${term}" "${MAX_PER_ALPHA2}"
              [ "$overall" -ge "$OVERALL_CAP_PER_JOB" ] && break
            done
            [ "$overall" -ge "$OVERALL_CAP_PER_JOB" ] && break
          done

          # 00..99
          for A in "${DIGITS[@]}"; do
            for B in "${DIGITS[@]}"; do
              term="${A}${B}"
              echo "=== ALPHA2 DD: ${term} @ ${BASE} ==="
              fetch_loop "${term}" "${MAX_PER_ALPHA2}"
              [ "$overall" -ge "$OVERALL_CAP_PER_JOB" ] && break
            done
            [ "$overall" -ge "$OVERALL_CAP_PER_JOB" ] && break
          done

          # a0..z9
          for A in "${LETTERS[@]}"; do
            for B in "${DIGITS[@]}"; do
              term="${A}${B}"
              echo "=== ALPHA2 LD: ${term} @ ${BASE} ==="
              fetch_loop "${term}" "${MAX_PER_ALPHA2}"
              [ "$overall" -ge "$OVERALL_CAP_PER_JOB" ] && break
            done
            [ "$overall" -ge "$OVERALL_CAP_PER_JOB" ] && break
          done

          # 0a..9z
          for A in "${DIGITS[@]}"; do
            for B in "${LETTERS[@]}"; do
              term="${A}${B}"
              echo "=== ALPHA2 DL: ${term} @ ${BASE} ==="
              fetch_loop "${term}" "${MAX_PER_ALPHA2}"
              [ "$overall" -ge "$OVERALL_CAP_PER_JOB" ] && break
            done
            [ "$overall" -ge "$OVERALL_CAP_PER_JOB" ] && break
          done

          # --- 4) COMMON-TERM SWEEP ---
          read -ra TERMS <<< "${COMMON_TERMS}"
          for T in "${TERMS[@]}"; do
            echo "=== TERM: ${T} @ ${BASE} ==="
            fetch_loop "${T}" "${MAX_PER_TERM}"
            [ "$overall" -ge "$OVERALL_CAP_PER_JOB" ] && break
          done

          # --- Per-job DEDUPE ---
          jq -s 'map({mid: (.modelId//.id//""), row: .}) | map(select(.mid != "")) | unique_by(.mid) | map(.row)' out/raw.jsonl > out/raw_dedup.json
          jq -c '.[]' out/raw_dedup.json > out/raw_dedup.jsonl
          rm -f out/raw_dedup.json
          echo "[info] job rows (deduped): $(wc -l < out/raw_dedup.jsonl)"

      - name: Upload chunk
        uses: actions/upload-artifact@v4
        with:
          name: chunk-${{ matrix.os }}-${{ matrix.base_alias }}
          path: out/raw_dedup.jsonl
          if-no-files-found: error

  merge:
    needs: [sweep]
    runs-on: ubuntu-latest
    timeout-minutes: 240
    steps:
      - uses: actions/checkout@v4
      - name: Install jq
        run: |
          sudo apt-get update -y
          sudo apt-get install -y jq

      - name: Download chunks
        uses: actions/download-artifact@v4
        with:
          path: chunks

      - name: Merge & dedupe chunks
        run: |
          set -euo pipefail
          mkdir -p out
          : > out/hf_models_raw.jsonl
          find chunks -type f -name '*.jsonl' -print0 | xargs -0 -I{} bash -c 'cat "{}" >> out/hf_models_raw.jsonl'
          echo "[info] merged lines: $(wc -l < out/hf_models_raw.jsonl)"
          jq -s 'map({mid: (.modelId//.id//""), row: .}) | map(select(.mid != "")) | unique_by(.mid) | map(.row)' out/hf_models_raw.jsonl > out/hf_models_raw_dedup.json
          jq -c '.[]' out/hf_models_raw_dedup.json > out/hf_models_raw_dedup.jsonl
          rm -f out/hf_models_raw_dedup.json
          echo "[info] after chunk dedupe: $(wc -l < out/hf_models_raw_dedup.jsonl)"

      - name: Mega ORG sweep (no-search), merge + final dedupe
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          ORGS1: "mistralai meta-llama Qwen deepseek-ai EleutherAI tiiuae google microsoft allenai nvidia stabilityai bigscience databricks togethercomputer baichuan-inc 01-ai xverse openaccess-ai-collective mlx-community Salesforce tencent OpenAssistant mlc-ai HuggingFaceH4 HuggingFaceM4 laion uclanlp stanfordnlp princeton-nlp OpenGVLab IDEA-CCNL openbmb baidu TencentARC kakaobrain ai-forever bigcode codellama codefuse-ai TheBloke bartowski NousResearch mlabonne philschmid RWKV declare-lab cerebras mosaicml nomic-ai openlm-research openbuddy openrouter aws-neuron OLMo-LM stabilityai-stablelm cerebras-GPT databricks-dolly vllm-project yandex ai21 SalesforceAIResearch Snowflake-Labs HuggingFace Open-Orca OpenBioLM BioMistral MedARC medalpaca OpenAccess-AI oobabooga KoboldAI koboldai teknium migtissera jartine lmsys tloen ray-project lightning-AI"
          ORGS2: "onnx openvino intel-neural-compressor tensorrt deepspeed bitsandbytes timdettmers huggingface-projects facebookresearch FAIR LAION-AI sberbank-ai yandex-research bigcode-project CodeLlama MetaLLama ResearchHub Flan-T5 GoogleResearch google-research OpenNMT OpenNMT-py pytorch TsinghuaKE THUDM ZhipuAI BAAI BAAI-InternLM InternLM OpenBMB Minfo-NLP UER-py UER Huang-Ling Alibaba-NLP alibaba-damo-academy alibaba bytedance ByteDance ByteDanceAI iflytek baichuan-inc2 Baichuan2 ZeroNLP NUS NLP4SG CMU-MLC USC-ICT Berkeley-NLP HKUST-NLP nyu-mll VITA-Group KAIST-AI NAVER NAVER-AI-Lab SKT-AI DeepMind LLaVA llava-ml alpaca-lora WizardLM orca-mini tiiuae-research mistralai-community mlx-examples microsoft/Phi-3 microsoft/Phi-4 meta-llama/Llama-3 meta-llama/Llama-2 Qwen/Qwen2 google/gemma bigscience/bloom stabilityai/stablelm togethercomputer/RedPajama-INCITE EleutherAI/gpt-j-6b EleutherAI/gpt-neo-125m facebook/opt-1.3b"
          ORGS3: "allenai OpenGVLab IDEA-CCNL COYO-SS HazyResearch echarlaix ybelkada patrickvonplaten NielsRogge sgugger stevhliu vmware intel mlcommons trl-lib TRL trl coqui-ai espnet ESPnet facebook fairseq openai anthropic cohere ai21 AI21Labs salesforce SalesforceAIResearch MIT MIT-HAN-LAB openflamingo OpenFlamingo open-llama Textualize Liger-Kernel RWKV RWKV-Runner nousresearch NousResearch alpindale Xwin-LM XVERSE deepseek-ai Qwen2 Qwen2.5 cerebras databricks snorkelai ORCA Dolphin Vicuna lmsys fastai fastai-community BaiduResearch Open-MMLab MMDet MMSegmentation MMPretrain OpenMMLab SkyworkAI airaria mlx-foundation"
          ORGS4: "OpenAccess-AI-collective OpenBioLM Open-Orca OpenAssistant OpenBMB OpenGVLab OpenNMT OpenNMT-py OpenFlamingo OpenMMLab OpenAI HuggingFaceH4 HuggingFaceM4 HuggingFace mlc-ai vllm-project stabilityai-stablelm CodeLlama MetaLLama InternLM BAAI-InternLM Minfo-NLP UER-py"
          ORGS5: "stabilityai-stablelm stabilityai openbmb bigcode-project thebloke TheBloke bertin-project cerebras-GPT cerebrasai TogetherAI togethercomputer RedPajama EleutherAI pythia MosaicML mosaicml Meta-Llama meta-llama meta FAIR-Research"
          ORGS6: "bigscience-workshop bigscience bloom HuggingFaceDatasets datasets-labs transformers diffusers peft accelerate optimum optimum-intel optimum-nvidia optimum-graphcore optimum-neuron optimum-habana safetensors sentence-transformers Laion-AI LAION research-backup"
          ORGS7: "openaccess-ai-collective openaccess-ai OpenAccess-AI-collective Open-LLMs open_llama open-llama alpacalora alpaca-lora WizardLM-team WizardLMX TiGAI TIGER-Lab TIGERLab TIGER-Lab-NUS UER-SG UER-py UER"
          ORGS8: "mlx-community mlx-examples mlx-foundation mlx-experiments apple mlc-ai-project mlc-ai web-llm WebLLM vllm-project vllm-project-inference xformers bitsandbytes-kernel k2-fsa kaldifeat espnet-oss nemo-toolkit nvidia-nemo NVIDIA-AI IPEX-Intel intel-aisaac"
        run: |
          set -euo pipefail
          BASE="https://huggingface.co/api/models"
          UA="COS-Frontier-GHA/9.0"
          AUTH=()
          if [ -n "${HF_TOKEN:-}" ]; then AUTH=(-H "Authorization: Bearer ${HF_TOKEN}"); fi
          ORGS="${ORGS1} ${ORGS2} ${ORGS3} ${ORGS4} ${ORGS5} ${ORGS6} ${ORGS7} ${ORGS8}"

          mkdir -p tmp
          : > tmp/author_append.jsonl
          for org in $ORGS; do
            url="${BASE}?limit=100&full=true&author=$(printf "%s" "$org" | jq -sRr @uri)"
            body=$(mktemp)
            code=$(curl -sS -m 60 -H "User-Agent: $UA" "${AUTH[@]}" "$url" -o "$body" -w '%{http_code}') || code=000
            if [ "$code" = "200" ]; then jq -c '.[]' < "$body" >> tmp/author_append.jsonl || true; else echo "[warn] HTTP $code author=$org"; fi
            rm -f "$body"
            sleep 0.06
          done

          (cat out/hf_models_raw_dedup.jsonl tmp/author_append.jsonl) > tmp/_merge2.jsonl
          jq -s 'map({mid: (.modelId//.id//""), row: .}) | map(select(.mid != "")) | unique_by(.mid) | map(.row)' <(jq -c '.' tmp/_merge2.jsonl) > out/hf_models_raw_dedup_final.json
          jq -c '.[]' out/hf_models_raw_dedup_final.json > out/hf_models_raw_dedup_final.jsonl
          rm -f out/hf_models_raw_dedup_final.json tmp/_merge2.jsonl tmp/author_append.jsonl
          echo "[info] FINAL rows: $(wc -l < out/hf_models_raw_dedup_final.jsonl)"

      - name: Make CSV (no Python)
        run: |
          set -euo pipefail
          echo "modelId,author,pipeline_tag,createdAt,lastModified,downloads,likes" > out/hf_models_dedup.csv
          jq -r '[ (.modelId // .id // ""), (.author // ""), (.pipeline_tag // ""), (.createdAt // ""), (.lastModified // ""), (.downloads // 0), (.likes // 0) ] | @csv' out/hf_models_raw_dedup_final.jsonl >> out/hf_models_dedup.csv
          echo "[info] CSV rows: $(($(wc -l < out/hf_models_dedup.csv)-1))"

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: hf-crawl-results
          path: |
            out/hf_models_raw.jsonl
            out/hf_models_raw_dedup_final.jsonl
            out/hf_models_dedup.csv
          if-no-files-found: error
