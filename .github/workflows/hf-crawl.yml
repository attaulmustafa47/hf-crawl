name: HF Crawl (all-in-one)

on:
  workflow_dispatch:
    inputs:
      max_per_tag:
        description: Cap per pipeline tag (rows)
        default: "50000"
      overall_cap:
        description: Overall safety cap (rows)
        default: "800000"
      tags:
        description: Comma-separated pipeline tags (search mode)
        default: "text-generation,text2text-generation,question-answering,summarization,translation,conversational,token-classification,text-classification,zero-shot-classification,table-question-answering,sentence-similarity,feature-extraction,fill-mask,image-classification,object-detection,image-segmentation,depth-estimation,image-to-text,text-to-image,image-to-image,unconditional-image-generation,super-resolution,visual-question-answering,document-question-answering,image-feature-extraction,automatic-speech-recognition,text-to-speech,audio-to-audio,voice-activity-detection,audio-classification,speaker-diarization,keyword-spotting,tabular-classification,tabular-regression,reinforcement-learning,time-series-forecasting"

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 420

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install jq
        run: |
          sudo apt-get update -y
          sudo apt-get install -y jq moreutils

      - name: Sweep with search+cursors (main endpoint)
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          MAX_PER_TAG: ${{ github.event.inputs.max_per_tag }}
          OVERALL_CAP: ${{ github.event.inputs.overall_cap }}
          TAGS: ${{ github.event.inputs.tags }}
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p out tmp
          : > out/hf_models_raw.jsonl

          BASE="https://huggingface.co/api/models"
          UA="COS-Frontier-GHA/5.0"
          AUTH=()
          if [ -n "${HF_TOKEN:-}" ]; then AUTH=(-H "Authorization: Bearer ${HF_TOKEN}"); fi

          IFS=',' read -ra TAGS_ARR <<< "${TAGS}"
          overall=0
          for TAG in "${TAGS_ARR[@]}"; do
            echo "=== TAG(main): ${TAG} ==="
            cursor=""
            fetched=0
            while :; do
              URL="${BASE}?limit=100&full=true&sort=downloads&direction=-1&search=$(printf "pipeline_tag:%s" "$TAG" | jq -sRr @uri)"
              [ -n "$cursor" ] && URL="${URL}&cursor=$(printf "%s" "$cursor" | jq -sRr @uri)"
              headers=$(mktemp); body=$(mktemp)
              code=$(curl -sS -m 60 -D "$headers" -H "User-Agent: $UA" "${AUTH[@]}" "$URL" -o "$body" -w '%{http_code}') || code=000
              if [ "$code" != "200" ]; then echo "[warn] HTTP $code (main)"; rm -f "$headers" "$body"; break; fi
              cnt=$(jq 'length' < "$body" || echo 0)
              if [ "$cnt" -eq 0 ]; then rm -f "$headers" "$body"; break; fi

              jq -c '.[]' < "$body" >> out/hf_models_raw.jsonl
              fetched=$((fetched + cnt)); overall=$((overall + cnt))
              cursor=$(awk -F': ' 'BEGIN{IGNORECASE=1} tolower($1)=="x-next-page-cursor"{gsub(/\r/,"",$2); print $2}' "$headers")

              rm -f "$headers" "$body"
              [ "$fetched" -ge "$MAX_PER_TAG" ] && echo "[cap] tag cap hit" && break
              [ "$overall" -ge "$OVERALL_CAP" ] && echo "[cap] overall cap hit" && break
              [ -z "$cursor" ] && break
              sleep 0.1
            done
            [ "$overall" -ge "$OVERALL_CAP" ] && break
          done

          echo "[info] RAW(main): $(wc -l < out/hf_models_raw.jsonl)"

      - name: Mirror retry if main is low; merge + dedupe
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          MAX_PER_TAG: ${{ github.event.inputs.max_per_tag }}
          OVERALL_CAP: ${{ github.event.inputs.overall_cap }}
          TAGS: ${{ github.event.inputs.tags }}
        shell: bash
        run: |
          set -euo pipefail
          UA="COS-Frontier-GHA/5.0"
          AUTH=()
          if [ -n "${HF_TOKEN:-}" ]; then AUTH=(-H "Authorization: Bearer ${HF_TOKEN}"); fi

          RAW_MAIN_LINES=$(wc -l < out/hf_models_raw.jsonl || echo 0)
          : > out/hf_models_raw_dedup.jsonl

          if [ "$RAW_MAIN_LINES" -lt 5000 ]; then
            echo "[info] main returned $RAW_MAIN_LINES (<5000). Trying mirrorâ€¦"
            BASE="https://hf-mirror.com/api/models"
            IFS=',' read -ra TAGS_ARR <<< "${TAGS}"
            overall=0
            : > tmp/hf_models_raw_mirror.jsonl
            for TAG in "${TAGS_ARR[@]}"; do
              echo "=== TAG(mirror): ${TAG} ==="
              cursor=""; fetched=0
              while :; do
                URL="${BASE}?limit=100&full=true&sort=downloads&direction=-1&search=$(printf "pipeline_tag:%s" "$TAG" | jq -sRr @uri)"
                [ -n "$cursor" ] && URL="${URL}&cursor=$(printf "%s" "$cursor" | jq -sRr @uri)"
                headers=$(mktemp); body=$(mktemp)
                code=$(curl -sS -m 60 -D "$headers" -H "User-Agent: $UA" "${AUTH[@]}" "$URL" -o "$body" -w '%{http_code}') || code=000
                if [ "$code" != "200" ]; then echo "[warn] HTTP $code (mirror)"; rm -f "$headers" "$body"; break; fi
                cnt=$(jq 'length' < "$body" || echo 0)
                if [ "$cnt" -eq 0 ]; then rm -f "$headers" "$body"; break; fi

                jq -c '.[]' < "$body" >> tmp/hf_models_raw_mirror.jsonl
                fetched=$((fetched + cnt)); overall=$((overall + cnt))
                cursor=$(awk -F': ' 'BEGIN{IGNORECASE=1} tolower($1)=="x-next-page-cursor"{gsub(/\r/,"",$2); print $2}' "$headers")

                rm -f "$headers" "$body"
                [ "$fetched" -ge "$MAX_PER_TAG" ] && echo "[cap] tag cap hit (mirror)" && break
                [ "$overall" -ge "$OVERALL_CAP" ] && echo "[cap] overall cap hit (mirror)" && break
                [ -z "$cursor" ] && break
                sleep 0.1
              done
              [ "$overall" -ge "$OVERALL_CAP" ] && break
            done
            (cat out/hf_models_raw.jsonl tmp/hf_models_raw_mirror.jsonl) > tmp/_merge_main_mirror.jsonl
            jq -s '
              map({mid: (.modelId//.id//""), row: .})
              | map(select(.mid != "")) 
              | unique_by(.mid)
              | map(.row)
            ' <(jq -c '.' tmp/_merge_main_mirror.jsonl) > out/_dedup.json
            jq -c '.[]' out/_dedup.json > out/hf_models_raw_dedup.jsonl
            rm -f out/_dedup.json tmp/_merge_main_mirror.jsonl
          else
            jq -s '
              map({mid: (.modelId//.id//""), row: .})
              | map(select(.mid != "")) 
              | unique_by(.mid)
              | map(.row)
            ' out/hf_models_raw.jsonl > out/_dedup.json
            jq -c '.[]' out/_dedup.json > out/hf_models_raw_dedup.jsonl
            rm -f out/_dedup.json
          fi

          echo "[info] DEDUP after search/mirror: $(wc -l < out/hf_models_raw_dedup.jsonl)"

      - name: Mega ORG sweep (no-search), merge + final dedupe
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        shell: bash
        run: |
          set -euo pipefail
          BASE="https://huggingface.co/api/models"
          UA="COS-Frontier-GHA/5.0"
          AUTH=()
          if [ -n "${HF_TOKEN:-}" ]; then AUTH=(-H "Authorization: Bearer ${HF_TOKEN}"); fi

          mkdir -p tmp
          : > tmp/author_append.jsonl

          # Large curated org/user list (hundreds). Add more names anytime.
          ORGS=$(cat <<'ORG'
mistralai
meta-llama
Qwen
deepseek-ai
EleutherAI
tiiuae
google
microsoft
allenai
nvidia
stabilityai
bigscience
databricks
togethercomputer
baichuan-inc
01-ai
xverse
openaccess-ai-collective
mlx-community
Salesforce
tencent
OpenAssistant
mlc-ai
HuggingFaceH4
HuggingFaceM4
laion
uclanlp
stanfordnlp
princeton-nlp
OpenGVLab
IDEA-CCNL
openbmb
baidu
TencentARC
kakaobrain
ai-forever
bigcode
codellama
codefuse-ai
TheBloke
bartowski
NousResearch
mlabonne
philschmid
RWKV
declare-lab
cerebras
mosaicml
nomic-ai
openlm-research
openbuddy
openrouter
aws-neuron
OLMo-LM
stabilityai-stablelm
cerebras-GPT
databricks-dolly
vllm-project
yandex
ai21
SalesforceAIResearch
Snowflake-Labs
HuggingFace
Open-Orca
OpenBioLM
BioMistral
MedARC
medalpaca
OpenAccess-AI
OpenAccess-AI-collective
oobabooga
KoboldAI
koboldai
teknium
migtissera
jartine
lmsys
tloen
ray-project
lightning-AI
onnx
openvino
intel-neural-compressor
tensorrt
deepspeed
bitsandbytes
timdettmers
huggingface-projects
facebookresearch
FAIR
LAION-AI
sberbank-ai
yandex-research
bigcode-project
CodeLlama
MetaLLama
ResearchHub
Flan-T5
GoogleResearch
google-research
OpenNMT
OpenNMT-py
pytorch
TsinghuaKE
THUDM
ZhipuAI
BAAI
BAAI-InternLM
InternLM
OpenBMB
Minfo-NLP
UER-py
UER
Huang-Ling
Alibaba-NLP
alibaba-damo-academy
alibaba
bytedance
ByteDance
ByteDanceAI
iflytek
baichuan-inc2
Baichuan2
ZeroNLP
NUS
NLP4SG
CMU-MLC
USC-ICT
Berkeley-NLP
HKUST-NLP
nyu-mll
VITA-Group
KAIST-AI
NAVER
NAVER-AI-Lab
SKT-AI
DeepMind
LLaVA
llava-ml
alpaca-lora
WizardLM
orca-mini
tiiuae-research
mistralai-community
mlx-examples
microsoft/Phi-3
microsoft/Phi-4
meta-llama/Llama-3
meta-llama/Llama-2
Qwen/Qwen2
google/gemma
bigscience/bloom
stabilityai/stablelm
togethercomputer/RedPajama-INCITE
EleutherAI/gpt-j-6b
EleutherAI/gpt-neo-125m
facebook/opt-1.3b
allenai
OpenBioLM
OpenGVLab
IDEA-CCNL
COYO-SS
HazyResearch
declare-lab
allenai
echarlaix
ybelkada
patrickvonplaten
NielsRogge
sgugger
stevhliu
philschmid
vmware
intel
nomic-ai
mosaicml
mlcommons
trl-lib
TRL
trl
tiiuae
coqui-ai
espnet
ESPnet
facebook
fairseq
openai
anthropic
cohere
ai21
AI21Labs
salesforce
SalesforceAIResearch
MIT
MIT-HAN-LAB
openflamingo
OpenFlamingo
open-llama
openlm-research
openrouter
openbuddy
Textualize
Liger-Kernel
EleutherAI
RWKV
RWKV-Runner
teknium
migtissera
TheBloke
bartowski
nousresearch
NousResearch
alpindale
Xwin-LM
XVERSE
01-ai
deepseek-ai
Qwen
Qwen2
Qwen2.5
cerebras
cerebras-GPT
databricks
databricks-dolly
snorkelai
ORCA
Open-Orca
Dolphin
WizardLM
Vicuna
lmsys
fastai
fastai-community
baidu
bytedance
tencent
alibaba
alibaba-damo-academy
TencentARC
BaiduResearch
ByteDanceAI
Open-MMLab
MMDet
MMSegmentation
MMPretrain
OpenMMLab
OpenGVLab
IDEA-CCNL
OpenBMB
BAAI
InternLM
SkyworkAI
airaria
HuggingFaceH4
HuggingFaceM4
mlx-community
mlx-examples
mlx-foundation
ORG
)

          for org in $ORGS; do
            url="${BASE}?limit=100&full=true&author=$(printf "%s" "$org" | jq -sRr @uri)"
            body=$(mktemp)
            code=$(curl -sS -m 60 -H "User-Agent: $UA" "${AUTH[@]}" "$url" -o "$body" -w '%{http_code}') || code=000
            if [ "$code" = "200" ]; then jq -c '.[]' < "$body" >> tmp/author_append.jsonl || true; else echo "[warn] HTTP $code author=$org"; fi
            rm -f "$body"
            sleep 0.06
          done

          (cat out/hf_models_raw_dedup.jsonl tmp/author_append.jsonl) > tmp/_merge2.jsonl
          jq -s '
            map({mid: (.modelId//.id//""), row: .})
            | map(select(.mid != "")) 
            | unique_by(.mid)
            | map(.row)
          ' <(jq -c '.' tmp/_merge2.jsonl) > out/hf_models_raw_dedup.json
          jq -c '.[]' out/hf_models_raw_dedup.json > out/hf_models_raw_dedup.jsonl
          rm -f out/hf_models_raw_dedup.json tmp/_merge2.jsonl tmp/author_append.jsonl

          echo "[info] FINAL rows: $(wc -l < out/hf_models_raw_dedup.jsonl)"

      - name: Make CSV (no Python)
        shell: bash
        run: |
          set -euo pipefail
          echo "modelId,author,pipeline_tag,createdAt,lastModified,downloads,likes" > out/hf_models_dedup.csv
          jq -r '
            [
              (.modelId // .id // ""),
              (.author // ""),
              (.pipeline_tag // ""),
              (.createdAt // ""),
              (.lastModified // ""),
              (.downloads // 0),
              (.likes // 0)
            ] | @csv
          ' out/hf_models_raw_dedup.jsonl >> out/hf_models_dedup.csv
          echo "[info] CSV rows: $(($(wc -l < out/hf_models_dedup.csv)-1))"

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: hf-crawl-results
          path: |
            out/hf_models_raw.jsonl
            out/hf_models_raw_dedup.jsonl
            out/hf_models_dedup.csv
          if-no-files-found: error
